{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc752f96",
   "metadata": {},
   "source": [
    "# CLASSIFICATION USING LE-NET ARCHITECTURE IN KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbd5bd",
   "metadata": {},
   "source": [
    "## Understanding LE-NET ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f0c01",
   "metadata": {},
   "source": [
    "LeNet-5 is a convolutional neural network (CNN) architecture that was developed by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. \n",
    "\n",
    "It is one of the pioneering CNN architectures and played a significant role in the development of deep learning and convolutional neural networks for image classification tasks. \n",
    "\n",
    "LeNet-5 was originally designed for handwritten digit recognition, specifically for recognizing characters on checks and other documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2708db",
   "metadata": {},
   "source": [
    "1. Input Layer: The network takes grayscale images of size 32x32 pixels as input. These images are typically normalized to have pixel values in the range  [0, 1].\n",
    "\n",
    "\n",
    "2. Convolutional Layer 1: The first convolutional layer applies six filters (also known as kernels) of size 5x5 to the input image. This results in six feature maps, each capturing different patterns or features in the input image.\n",
    "\n",
    "\n",
    "3. Average Pooling Layer 1: This layer performs average pooling on each of the feature maps generated by the first convolutional layer. The pooling is done using a 2x2 window with a stride of 2, which reduces the spatial dimensions of the feature maps by half.\n",
    "\n",
    "\n",
    "4. Convolutional Layer 2: The second convolutional layer applies sixteen 5x5 filters to the output of the first pooling layer. This produces sixteen new feature maps.\n",
    "\n",
    "\n",
    "5. Average Pooling Layer 2: Similar to the first pooling layer, the second pooling layer performs average pooling using a 2x2 window with a stride of 2.\n",
    "\n",
    "\n",
    "6. Flatten Layer: The output of the second pooling layer is flattened into a 1D vector. This vector serves as the input to the fully connected layers.\n",
    "\n",
    "\n",
    "7. Fully Connected Layer 1: This layer has 120 neurons and is fully connected to the flattened output of the previous layer. It applies a traditional fully connected operation followed by a non-linear activation function, typically a sigmoid or hyperbolic tangent (tanh) function.\n",
    "\n",
    "\n",
    "8. Fully Connected Layer 2: This layer has 84 neurons and is also fully connected to the previous layer. Similarly, it applies a non-linear activation function.\n",
    "\n",
    "\n",
    "9. Output Layer: The final fully connected layer has 10 neurons, each corresponding to one of the possible digits (0 to 9). It uses a softmax activation function to produce the probability distribution over the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06434e93",
   "metadata": {},
   "source": [
    "LeNet-5 is a very efficient convolutional neural network for handwritten character recognition.\n",
    "\n",
    "\n",
    "Convolutional neural networks can make good use of the structural information of images.\n",
    "\n",
    "\n",
    "The convolutional layer has fewer parameters, which is also determined by the main characteristics of the convolutional layer, that is, local connection and shared weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa1c40",
   "metadata": {},
   "source": [
    "## Basic Code Implementation for LE-NET 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db898f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f801d97",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38d2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset and perform splitting\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71be3c9",
   "metadata": {},
   "source": [
    "### Image Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b5767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peforming reshaping operation\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# One Hot Encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd04064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7da0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c3fe28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68a974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23276fc",
   "metadata": {},
   "source": [
    "### LeNet Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363c0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model Architecture\n",
    "model = Sequential()\n",
    "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 feature maps. The size of each feature map is 32−5 + 1 = 2832−5 + 1 = 28.\n",
    "# That is, the number of neurons has been reduced from 10241024 to 28 ∗ 28 = 784 28 ∗ 28 = 784.\n",
    "# Parameters between input layer and C1 layer: 6 ∗ (5 ∗ 5 + 1)\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "# The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node matrix.\n",
    "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 14 * 14 * 6.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
    "# The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "# The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2, so the output matrix size of this layer is 5 * 5 * 16.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
    "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
    "# The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 = 48120 parameters.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
    "model.add(Dense(84, activation='relu'))\n",
    "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
    "model.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc29ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x212e5c036d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b40fd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01afc0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 20s 38ms/step - loss: 0.3553 - accuracy: 0.8936 - val_loss: 0.1723 - val_accuracy: 0.9466\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 0.1029 - accuracy: 0.9686 - val_loss: 0.0719 - val_accuracy: 0.9773\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0710 - accuracy: 0.9780 - val_loss: 0.0530 - val_accuracy: 0.9829\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0557 - accuracy: 0.9831 - val_loss: 0.0421 - val_accuracy: 0.9863\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.0377 - val_accuracy: 0.9884\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0400 - accuracy: 0.9874 - val_loss: 0.0434 - val_accuracy: 0.9847\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 0.0352 - accuracy: 0.9890 - val_loss: 0.0373 - val_accuracy: 0.9887\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.0359 - val_accuracy: 0.9883\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 18s 37ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.0450 - val_accuracy: 0.9861\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0240 - accuracy: 0.9922 - val_loss: 0.0386 - val_accuracy: 0.9891\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.0483 - val_accuracy: 0.9858\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0311 - val_accuracy: 0.9898\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 0.0342 - val_accuracy: 0.9897\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.0371 - val_accuracy: 0.9896\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0407 - val_accuracy: 0.9887\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.0355 - val_accuracy: 0.9898\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.0455 - val_accuracy: 0.9891\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0411 - val_accuracy: 0.9898\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.0449 - val_accuracy: 0.9884\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0471 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212e5faf1f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f313de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0471 - accuracy: 0.9883\n",
      "Test Loss: 0.04711113125085831\n",
      "Test accuracy: 0.9883000254631042\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8487ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e51c78ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 6)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7fc7929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filters_layer = model.layers[0]  # Assuming the first layer is a Conv2D layer\n",
    "filters, biases = filters_layer.get_weights()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(filters.shape[3]):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(filters[:, :, 0, i], cmap='pink')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
